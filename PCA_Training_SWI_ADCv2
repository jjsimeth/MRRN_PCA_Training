# -*- coding: utf-8 -*-
"""
Created on Fri Oct 27 13:41:09 2023

@author: SimethJ
"""



# -*- coding: utf-8 -*-
"""
Created on Wed Oct 25 11:45:59 2023

@author: SimethJ

Updated inference function that will process all the nifti files in a specified
folder (default: )

assumes nifti files with units 1e-3 mm^2/s

"""

#import nibabel as nib
import torch as t
from torch.cuda.amp import GradScaler, autocast
import numpy as np
# from PIL import Image
# import torchvision.transforms as transforms

#import matplotlib.pyplot as plt
import os
import SimpleITK as sitk
import torch.multiprocessing
import datetime

import torch.utils.data
from options.train_options import TrainOptions
from models.models import create_model
# from util import util
from PCA_DIL_inference_utils import sliding_window_inference 

opt = TrainOptions().parse()
opt.isTrain=True

# from pathlib import Path
from glob import glob

from typing import Tuple
import monai
from monai.handlers.utils import from_engine
from monai.data import DataLoader, create_test_image_3d,PatchDataset
from monai.data import decollate_batch


#from monai.inferers import SliceInferer
#from monai.metrics import DiceMetric
from monai.transforms import (
    OneOf,
    Compose,
    EnsureChannelFirstd,
    EnsureTyped,
    LoadImaged,
    RandRotate90d,
    Resized,
    ScaleIntensityd,
    Spacingd,
    Orientationd,
    SqueezeDimd,
    ResizeWithPadOrCropd,
    CropForegroundd,
    RandRotated,
    CenterSpatialCropd,
    Transposed,
    ResampleToMatchd,
    RandSpatialCropd,
    Invertd,
    AsDiscreted,
    RandGaussianNoised,
    RandGaussianSmoothd,
    ScaleIntensityRangePercentilesd,
    RandAdjustContrastd,
    RandHistogramShiftd,
    RandBiasFieldd,
    RandFlipd,
    RandAxisFlipd,
    RandSpatialCropSamplesd,
    RandSimulateLowResolutiond,
    RandGaussianSharpend,
    RandRicianNoised,
    RandGibbsNoised,
    RandKSpaceSpikeNoised,
    RandCoarseDropoutd,
    RandRotate90d,
)





def copy_info(src, dst):

    dst.SetSpacing(src.GetSpacing())
    dst.SetOrigin(src.GetOrigin())
    dst.SetDirection(src.GetDirection())

    return dst

def partial_mixup(input: torch.Tensor,
                  gamma: float,
                  indices: torch.Tensor
                  ) -> torch.Tensor:
    if input.size(0) != indices.size(0):
        raise RuntimeError("Size mismatch!")
    perm_input = input[indices]
    return input.mul(gamma).add(perm_input, alpha=1 - gamma)


def mixup(input: torch.Tensor,
          target: torch.Tensor,
          gamma: float,
          ) -> Tuple[torch.Tensor, torch.Tensor]:
    indices = torch.randperm(input.size(0), device=input.device, dtype=torch.long)
    return partial_mixup(input, gamma, indices), partial_mixup(target, gamma, indices)




def scale_ADC(image):
    adc_max=image.data.amax()
    adc_max=adc_max.data.cpu().numpy()
    
    
    if adc_max<0.05:# input likely in  mm^2/s, 
        print('Confirm units of ADC: input probably incorrectly in mm^2/s, normalizing with that assumption')
        multiplier=1e6
    elif adc_max<50:#input likely in  1e-3 mm^2/s, 
        print('Confirm units of ADC: input probably incorrectly in 1e-3  mm^2/s, normalizing with that assumption')
        multiplier=1
    elif adc_max<50000:#input likely in  1e-6 mm^2/s, 
        multiplier=1e-3
        #print('Confirm units of ADC: input probably incorrectly in 1e-6  mm^2/s, normalizing with that assumption')
    else:
        print('Confirm units of ADC: values too large to be 1e-6  mm^2/s')
    adc=2*(image.data*multiplier)/3.5-1
    adc=t.clip(adc,min=-1.0,max=1.0)
    
    return adc




def find_file_with_string(folder_path, target_string):
    # Get the list of files in the specified folder
    files = os.listdir(folder_path)
    # Iterate through the files and find the first file containing the target string
    for file in files:
        if target_string in file:
            # If the target string is found in the file name, return the file
            return file
    # If no matching file is found, return None
    return None
    
def choose_weight_strategy_params(opt):
    if opt.nslices >= 3:
      # With triangular and falloff_rate of 0.5, each middle slice is more important but other slices keep relevance
      # the weight distribution for 3 slices is [0.66,1.0.66]
      # the weight dsitrbution for 5 slices is [0.6,0.8,1.0,0.8,0.6] 
      strategy, params = 'triangular', {'falloff_rate': 0.5}

      #strategy, params = 'gaussian', {'sigma': opt.nslices/4}
      #strategy, params = 'uniform', None
      
    return strategy, params
      
    
def train_with_multi_slice_mixup(inputs, labels, model, opt):
    """
    Train model with multi-slice approach using mixup and gradient accumulation
    
    Args:
        inputs: Input tensor (B, C, H, W)
        labels: Label tensor (B, S, H, W) where S is number of slices
        model: The MRRN_Segmentor model
        opt: Options containing hyperparameters
    
    Returns:
        Aggregated loss value
    """
    # Multi slice approach
    middle_slice_idx = np.int32((opt.nslices-1)/2)
    
    strategy, params = choose_weight_strategy_params(opt)
    
    # Create weights for slices
    slice_weights = create_slice_weights(
        opt.nslices, 
        strategy=strategy, 
        params=params,
    )
    
    # Zero gradients once
    model.optimizer_Seg_A.zero_grad()
    
    # Generate mixup parameters once for consistency
    beta_val = np.random.beta(opt.mixup_betadist, opt.mixup_betadist)
    indices = torch.randperm(inputs.size(0), device=inputs.device, dtype=torch.long)
    mixed_inputs = partial_mixup(inputs, beta_val, indices)
    
    # Process all slices with appropriate weights
    total_weighted_loss = 0
    total_weight = 0
    for slice_idx in range(opt.nslices):
        slice_labels = labels[:, slice_idx, :, :]
        slice_labels = torch.clamp(slice_labels, 0.001, 0.999).float()
        mixed_labels = partial_mixup(slice_labels, beta_val, indices)
        
        # Forward pass without gradient update
        model.set_input_sep(mixed_inputs, mixed_labels)
        model.forward()
        slice_loss = model.cal_seg_loss(model.netSeg_A, mixed_inputs, mixed_labels)
        
        # Weight and accumulate loss
        total_weighted_loss += slice_loss * slice_weights[slice_idx]
        total_weight += slice_weights[slice_idx]
    
    # Single backward pass with normalized loss
    avg_loss = total_weighted_loss / total_weight
    avg_loss.backward()
    torch.nn.utils.clip_grad_norm_(model.netSeg_A.parameters(), max_norm=15.0)
    model.optimizer_Seg_A.step()
    
    return avg_loss
    
def create_slice_weights(num_slices, strategy='gaussian', params=None):
    """
    Generate weights for slices in a 3D volume.
    
    Args:
        num_slices (int): Total number of slices in the volume.
        strategy (str): Weighting strategy, options:
            - 'gaussian': Gaussian weights centered on middle slice
            - 'triangular': Linear falloff from center
            - 'uniform': Equal weight to all slices
        params (dict, optional): Strategy-specific parameters:
            - For 'gaussian': {'sigma': float} - standard deviation
            - For 'triangular': {'falloff_rate': float} - steepness of falloff
    
    Returns:
        torch.Tensor: Tensor of weights for each slice, normalized to sum to num_slices
    """
    import torch
    import numpy as np
    
    # Default parameters
    if params is None:
        params = {}
    
    # Create empty weights tensor
    weights = torch.ones(num_slices)
    
    # Middle slice index (for centered distributions)
    middle_idx = (num_slices - 1) / 2
    
    # Generate weights based on strategy
    if strategy == 'uniform':
        # Equal weight to all slices
        pass  # weights already initialized to ones
        
    elif strategy == 'gaussian':
        # Gaussian distribution centered on middle slice
        sigma = params.get('sigma', num_slices / 4)  # Default: 1/4 of volume width
        
        # Generate indices
        indices = torch.arange(num_slices, dtype=torch.float32)
        
        # Compute Gaussian weights
        weights = torch.exp(-((indices - middle_idx) ** 2) / (2 * sigma ** 2))
        
    elif strategy == 'triangular':
        # Linear falloff from center
        falloff_rate = params.get('falloff_rate', 0.5)
        
        # Generate indices
        indices = torch.arange(num_slices, dtype=torch.float32)
        
        # Compute triangular weights
        weights = 1.0 - falloff_rate * torch.abs(indices - middle_idx) / (num_slices / 2)
        weights = torch.clamp(weights, min=0.1)  # Prevent zero weights
    else:
        raise ValueError(f"Unknown weighting strategy: {strategy}")
    
    # Normalize weights to sum to num_slices (preserves average gradient scale)
    weights = weights * (num_slices / weights.sum())
    
    return weights

mr_paths = []
seg_paths = []

#Define input dimensions and resolution for inference model 
PD_in=np.array([0.6250, 0.6250, 3]) # millimeters
DIM_in=np.array([128,128,opt.nslices]) # 128x128 5 Slices
nmodalities=1

root_dir=os.getcwd()
opt.nchannels=opt.nslices*nmodalities

model = create_model(opt) 

model_path = os.path.join(root_dir,'deep_model','MRRNDS_model') #load weights

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#model.load_MR_seg_A(model_path) #use weights
# model.to(device)

#freeze all layers
#for param in model.netSeg_A.parameters():
#        param.requires_grad = False

                
#impath = os.path.join(path,'nii_vols') #load weights
impath = os.path.join(root_dir,'training_data_v3') #load weights
valpath = os.path.join(root_dir,'validation_data_v3') #load weights


#unfreeze final layer and Channel input block for finetuning
# model.netSeg_A.CNN_block1.requires_grad_(True)
# model.netSeg_A.out_conv.requires_grad_(True)
# model.netSeg_A.out_conv1.requires_grad_(True)
# if opt.deeplayer>0:
#     model.netSeg_A.deepsupconv.requires_grad_(True)
dest_path= os.path.join(root_dir,opt.name) 
wt_path= os.path.join(root_dir,opt.name,'ct_seg_val_loss.csv')
if not os.path.exists(dest_path):
    os.makedirs(dest_path)
fd_results = open(wt_path, 'w')
fd_results.write('train loss, seg accuracy,\n')

results_path=os.path.join(root_dir,opt.name,'val_results')
if not os.path.exists(results_path):
    os.makedirs(results_path)   
seg_path=os.path.join(root_dir,opt.name,'Output_segs')
if not os.path.exists(seg_path):
    os.makedirs(seg_path)    


types = ('ProstateX*_ep2d_diff_*.nii.gz', 'MSK_MR_*_ADC.nii.gz', '*_ivim_adc.nii') # the tuple of file types
images=[]
for fname in types:
   images.extend(glob(os.path.join(impath, fname)))
images = sorted(images)

types = ('ProstateX*Finding*t2_tse_tra*_ROI.nii.gz', 'MSK_MR_*_GTV.nii.gz', '*_LES*.nii') # the tuple of file types
segs=[]
for fname in types:
   segs.extend(glob(os.path.join(impath, fname)))
segs = sorted(segs)
   
types = ('ProstateX*_t2_tse*.nii.gz', 'MSK_MR_*T2w.nii.gz','*_tt2.nii') # the tuple of file types
images_t2w=[]
for fname in types:
   images_t2w.extend(glob(os.path.join(impath, fname)))
images_t2w = sorted(images_t2w)

types = ('*_pros_bas.nii', 'mskpros.nii') # the tuple of file types
pros_segs=[]
for fname in types:
   pros_segs.extend(glob(os.path.join(impath, fname)))
pros_segs = sorted(pros_segs)

# #val_images = (glob(os.path.join(valpath, "*_ep2d_diff_*.nii.gz"))+glob(os.path.join(impath, "*ADC_p*.nii"))) #adc keywords from filename

# #val_images = (glob(os.path.join(valpath, "*_ep2d_diff_*.nii.gz"))+glob(os.path.join(impath, "*ADC_p*.nii"))) #adc keywords from filename
# val_images = glob(os.path.join(valpath, "*_ep2d_diff_*.nii.gz")) #adc keywords from filename
# val_images.extend(glob(os.path.join(impath, "*ADC_p*.nii")))
# val_images = sorted(val_images)
# #segs = sorted(glob(os.path.join(impath, "*_ADC_ROI*.nii.gz")))
# #val_segs = sorted(glob(os.path.join(valpath, "*t2_tse_tra*_ROI.nii.gz"))+glob(os.path.join(impath, "*T2w_p*.nii")))

# val_segs = glob(os.path.join(valpath, "*t2_tse_tra*_ROI.nii.gz")) #adc keywords from filename
# val_segs.extend(glob(os.path.join(impath, "*T2w_p*.nii")))
# val_segs = sorted(val_segs)


# # images_t2w = sorted(glob(os.path.join(root_dir, "t2w*.nii.gz")))
# #val_images_t2w = sorted(glob(os.path.join(valpath, "*_t2_tse*.nii.gz"))+glob(os.path.join(impath, "*GTV_p*.nii"))) #t2w keywords from filename
# val_images_t2w = glob(os.path.join(valpath, "*_t2_tse*.nii.gz")) #adc keywords from filename
# val_images_t2w.extend(glob(os.path.join(impath, "*GTV_p*.nii")))
# val_images_t2w = sorted(val_images_t2w)




types = ('ProstateX*_ep2d_diff_*.nii.gz', 'MSK_MR_*_ADC.nii.gz', '*_ivim_adc.nii') # the tuple of file types
val_images=[]
for fname in types:
   val_images.extend(glob(os.path.join(valpath, fname)))
val_images = sorted(val_images)

types = ('ProstateX*Finding*t2_tse_tra*_ROI.nii.gz', 'MSK_MR_*_GTV.nii.gz', '*_LES*.nii' ) # the tuple of file types
val_segs=[]
for fname in types:
   val_segs.extend(glob(os.path.join(valpath, fname)))
val_segs = sorted(val_segs)
 

types = ('ProstateX*_t2_tse*.nii.gz', 'MSK_MR_*T2w.nii.gz', '*_tt2.nii') # the tuple of file types
val_images_t2w=[]
for fname in types:
   val_images_t2w.extend(glob(os.path.join(valpath, fname)))
val_images_t2w = sorted(val_images_t2w)

types = ('*_pros_bas.nii', 'mskpros.nii') # the tuple of file types
val_pros_segs=[]
for fname in types:
   val_pros_segs.extend(glob(os.path.join(valpath, fname)))
val_pros_segs = sorted(val_pros_segs)



train_files = [{"img": img, "seg": seg, "t2w": t2w, "prost": pros} for img, seg, t2w, pros in zip(images, segs, images_t2w, pros_segs)] #first n_train to training
val_files = [{"img": img, "seg": seg, "t2w": t2w, "prost": pros} for img, seg, t2w, pros in zip(val_images, val_segs, val_images_t2w, val_pros_segs)] #last  n_val to validation



# print(train_files)
# print(val_files)


train_transforms = Compose(
    [
            LoadImaged(keys=["img", "t2w", "prost","seg"]),
            EnsureChannelFirstd(keys=["img", "t2w","prost", "seg"]),
            Orientationd(keys=["img", "t2w","prost", "seg"], axcodes="RAS"),

            # ResampleToMatchd(keys=["img"],
            #                  key_dst="t2w",
            #                  mode="bilinear"),
            # ResampleToMatchd(keys=["seg"],
            #                  key_dst="t2w",
            #                  mode="nearest"),
            ResampleToMatchd(keys=["img","prost","seg"],
                             key_dst="t2w",
                             mode=("bilinear","nearest", "nearest")),
            Spacingd(keys=["img", "t2w", "prost","seg"],
                     pixdim=(PD_in[0], PD_in[1], PD_in[2]),
                     mode=("bilinear", "bilinear","nearest", "nearest")),
            #CenterSpatialCropd(keys=["img", "t2w", "seg"], roi_size=[200, 200, -1]),
            
            CenterSpatialCropd(keys=["img","t2w", "prost","seg"], roi_size=[256, 256,-1]),
            
            CropForegroundd(keys=["img", "t2w", "prost", "seg"], source_key="seg", margin=[256, 256, opt.extra_neg_slices + (opt.nslices - 1) / 2],allow_smaller=True),
            CropForegroundd(keys=["img", "t2w", "prost", "seg"], source_key="prost", margin=[64, 64, opt.extra_neg_slices + (opt.nslices - 1) / 2]),
            
            
            #CropForegroundd(keys=["img", "t2w", "prost", "seg"], source_key="prost", margin=[64, 64, opt.extra_neg_slices + (opt.nslices - 1) / 2]),
            #CropForegroundd(keys=["img", "t2w", "prost", "seg"], source_key="prost", margin=[64, 64, opt.extra_neg_slices + (opt.nslices - 1) / 2]),
            
            
            # ScaleIntensityd(keys="t2w",minv=-1.0, maxv=1.0),
                     
            # Intensity and texture modifications
            OneOf([
                RandAdjustContrastd(keys=["img"], prob=0.15,  gamma=(0.75, 2.0)),
                RandHistogramShiftd(keys=["img"], prob=0.15, num_control_points=30),
                RandBiasFieldd(keys=["img"], prob=0.15, coeff_range=(0.1, 0.25)),
                RandGaussianSmoothd(keys=["img"], prob=0.1),
                RandGaussianSharpend(keys=["img"], prob=0.1),
            ]),
                        
            # MRI-specific noise and cutout
            OneOf([
                RandRicianNoised(keys=["img"], prob=0.15, std=0.4, relative=True, channel_wise=True),
                RandGibbsNoised(keys=["img"], prob=0.15, alpha=(0.25, 0.75)),
                RandKSpaceSpikeNoised(keys=["img"], prob=0.15, intensity_range=(0.75, 2.5)),
                RandCoarseDropoutd(keys=["img"], prob=0.1, holes=14, spatial_size=15),
                RandCoarseDropoutd(keys=["img"], prob=0.1, holes=6, spatial_size=40),
            ]),
            
            
            #RandSimulateLowResolutiond(keys=["img", "t2w"], prob=0.15),
            ScaleIntensityRangePercentilesd(keys=["img"], lower=0, upper=99, b_min=-1.0, b_max=1.0, clip=True,channel_wise=True),
            ScaleIntensityRangePercentilesd(keys=["t2w"], lower=0, upper=99, b_min=-1.0, b_max=1.0, clip=True,channel_wise=True),
            #RandCropByPosNegLabeld(keys=["img", "t2w", "prost", "seg"], label_key="seg", spatial_size=(128, 128,opt.nslices)),
            
            # In plane axial/transversal rotations in rads up to 90 degs
            RandRotated(keys=["img", "t2w", "prost", "seg"], prob=1.0, range_x=0.0, range_y=0.0, range_z=1.6, mode=("bilinear", "bilinear","nearest", "nearest")),
            
            # Out of plane rotations in rads up to 4 degs for regularisation
            OneOf([
              RandRotated(keys=["img", "t2w", "prost", "seg"], prob=0.15, range_x=0.085, range_y=0.0, range_z=0.0, mode=("bilinear", "bilinear","nearest", "nearest")),
              RandRotated(keys=["img", "t2w", "prost", "seg"], prob=0.15, range_x=0.0, range_y=0.085, range_z=0.0, mode=("bilinear", "bilinear","nearest", "nearest")),
            ]),
            
            CenterSpatialCropd(keys=["img","t2w", "prost","seg"], roi_size=[160, 160,-1]),
            #CropForegroundd(keys=["img", "t2w", "prost", "seg"], source_key="seg", margin=[96, 96, opt.extra_neg_slices + (opt.nslices - 1) / 2]),
            
            
            #RandSpatialCropSamplesd(keys=["img", "t2w", "prost", "seg"], num_samples=1, roi_size=(128, 128,opt.nslices),random_size=False,random_center=True),
            RandSpatialCropd(keys=["img", "t2w", "prost",  "seg"], roi_size=(128, 128, opt.nslices), random_size=False,random_center =True),
            
            RandRotate90d(keys=["img", "t2w", "prost", "seg"], prob=0.75, max_k=3, spatial_axes=(1,0),),
            RandAxisFlipd(keys=["img", "t2w", "prost", "seg"], prob=0.5),
            
           
            Transposed(keys=["img", "t2w",  "prost", "seg"], indices=[3, 2, 1, 0]),
            SqueezeDimd(keys=["img", "t2w",  "prost", "seg"], dim=-1),
            EnsureTyped(keys=["img", "t2w",  "prost", "seg"]),

    ]
)


val_transforms = Compose(
    [
        LoadImaged(keys=["img", "t2w", "prost","seg"]),
        EnsureChannelFirstd(keys=["img", "t2w","prost", "seg"]),
        Orientationd(keys=["img", "t2w","prost", "seg"], axcodes="RAS"),

        # ResampleToMatchd(keys=["img"],
        #                  key_dst="t2w",
        #                  mode="bilinear"),
        # ResampleToMatchd(keys=["seg"],
        #                  key_dst="t2w",
        #                  mode="nearest"),
        ResampleToMatchd(keys=["t2w","prost","seg"],
                         key_dst="img",
                         mode=("bilinear","nearest", "nearest")),
        Spacingd(keys=["img", "t2w", "prost","seg"],
                 pixdim=(PD_in[0], PD_in[1], PD_in[2]),
                 mode=("bilinear", "bilinear","nearest", "nearest")),
        #CenterSpatialCropd(keys=["img", "t2w", "seg"], roi_size=[200, 200, -1]),
        
        CenterSpatialCropd(keys=["img","t2w", "prost","seg"], roi_size=[256, 256,-1]),
        CropForegroundd(keys=["img", "t2w", "prost", "seg"], source_key="seg", margin=[256, 256, opt.extra_neg_slices + (opt.nslices - 1) / 2],allow_smaller=True),
        CropForegroundd(keys=["img", "t2w", "prost", "seg"], source_key="prost", margin=[64, 64, opt.extra_neg_slices + (opt.nslices - 1) / 2]),
        
        #CropForegroundd(keys=["img", "t2w", "prost", "seg"], source_key="prost", margin=[64, 64, opt.extra_neg_slices + (opt.nslices - 1) / 2]),
        CenterSpatialCropd(keys=["img","t2w", "prost","seg"], roi_size=[142, 142,-1]),
            
        ScaleIntensityRangePercentilesd(keys=["img"], lower=0, upper=99, b_min=-1.0, b_max=1.0, clip=True,channel_wise=True),
        ScaleIntensityRangePercentilesd(keys=["t2w"], lower=0, upper=99, b_min=-1.0, b_max=1.0, clip=True,channel_wise=True),
        EnsureTyped(keys=["img","t2w","prost",  "seg"]),
    ]
)

post_transforms = Compose([
        EnsureTyped(keys="pred"),
        Invertd(
            keys="pred",
            transform=val_transforms,
            orig_keys="img",
            meta_keys="pred_meta_dict",
            orig_meta_keys="image_meta_dict",
            meta_key_postfix="meta_dict",
            nearest_interp=True,
            to_tensor=True,
        ),
        AsDiscreted(keys="pred", argmax=False),

    ])


# 3D dataset with preprocessing transforms
#train_ds = monai.data.CacheDataset(data=train_files, transform=train_transforms)
train_ds = monai.data.ShuffleBuffer(data=train_files, transform=train_transforms, seed=0, buffer_size=1024)
#train_ds = monai.data.ShuffleBuffer(patch_ds, seed=0) #patch based trainer



val_ds = monai.data.CacheDataset(data=val_files, transform=val_transforms)


train_loader = DataLoader(
    train_ds,
    batch_size=opt.batchSize,
    num_workers=9,
    pin_memory=torch.cuda.is_available(),
    drop_last=True
)
val_loader = DataLoader(
    val_ds,
    batch_size=1,
    num_workers=8,
    pin_memory=torch.cuda.is_available(),
)


check_data = monai.utils.misc.first(train_loader)
print("first patch's shape: ", check_data["img"].shape, check_data["seg"].shape)

def save_training_samples(loader, num_samples=30, save_dir=dest_path, modality=None):
    """
    Save the first n training samples with transformations applied
    
    Args:
        loader: DataLoader to iterate through
        num_samples: Maximum number of samples to save
        save_dir: Directory to save samples to
        modality: Selected modality to save, if None is selected, all modalities will be saved
    """
    print(f"Saving first {num_samples} training samples with transformations...")
    
    saved_count = 0
    # Create a temporary loader with batch size 1 to properly save individual samples
    temp_loader = DataLoader(
        train_ds,
        batch_size=1,
        num_workers=4,
        pin_memory=torch.cuda.is_available(),
        shuffle=False  # Important: no shuffling to capture first n samples
    )
    
    for batch_data in temp_loader:
        # Check if modality is specified or save all modalities
        if modality is None or modality == 'adc':
            if "img" in batch_data:
                # Save ADC image
                sitk.WriteImage(
                    sitk.GetImageFromArray(batch_data["img"][0].cpu().numpy()), 
                    os.path.join(save_dir, f'TRAINING_DEBUG_adc_sample{saved_count+1}.nii.gz')
                )
        
        if modality is None or modality == 'perf':
            if "perf" in batch_data:
                # Save perfusion image
                sitk.WriteImage(
                    sitk.GetImageFromArray(batch_data["perf"][0].cpu().numpy()),
                    os.path.join(save_dir, f'TRAINING_DEBUG_perf_sample{saved_count+1}.nii.gz')
                )
        
        if modality is None or modality == 't2w':
            if "t2" in batch_data:
                # Save T2 image
                sitk.WriteImage(
                    sitk.GetImageFromArray(batch_data["t2w"][0].cpu().numpy()),
                    os.path.join(save_dir, f'TRAINING_DEBUG_t2_sample{saved_count+1}.nii.gz')
                )
        
        if modality is None or modality == 'label':
            if "seg" in batch_data:
                # Save label segmentation mask
                sitk.WriteImage(
                    sitk.GetImageFromArray(batch_data["seg"][0].cpu().numpy()),
                    os.path.join(save_dir, f'TRAINING_DEBUG_SEG_sample{saved_count+1}.nii.gz')
                )
        
        if modality is None or modality == 'pros':
            if "pros_seg" in batch_data:
                # Save prostate segmentation mask
                sitk.WriteImage(
                    sitk.GetImageFromArray(batch_data["prost"][0].cpu().numpy()),
                    os.path.join(save_dir, f'TRAINING_DEBUG_PROS_sample{saved_count+1}.nii.gz')
                )
        
        saved_count += 1
        if saved_count >= num_samples:
            break
    
    print(f"Saved {saved_count} training samples to {save_dir}")


# Call the function to save training samples before starting training
save_training_samples(train_loader, num_samples=3)
save_training_samples(train_loader, num_samples=70, modality='adc')

#sitk.WriteImage(sitk.GetImageFromArray(check_data["t2w"].cpu()[0,:,:,:]),  os.path.join(dest_path,'TRAINING_DEBUG_t2w.nii.gz'))
#sitk.WriteImage(sitk.GetImageFromArray(check_data["img"].cpu()[0,:,:,:]),  os.path.join(dest_path,'TRAINING_DEBUG_adc.nii.gz'))
#sitk.WriteImage(sitk.GetImageFromArray(check_data["seg"].cpu()[0,:,:,:]),  os.path.join(dest_path,'TRAINING_DEBUG_SEG.nii.gz'))


num_epochs=opt.niter+opt.niter_decay
epoch_loss_values = []
best_dice=0
best_epoch=0
mixup_ab=opt.mixup_betadist
for epoch in range(num_epochs+1):
    batch_loss_values = []
    lr=model.get_curr_lr()
    current_datetime = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M")
    print(f'Current epoch: {epoch}/{num_epochs}, datetime: {current_datetime}, lr: {lr:.6f}')
    epoch_loss, step = 0, 0
    for batch_data in train_loader:
        step += 1
        
        adc, labels, t2w, prost = batch_data["img"].to(device), batch_data["seg"].to(device), batch_data["t2w"].to(device), batch_data["prost"].to(device)
        
        inputs=adc

        
        if opt.nslices > 1:
            # Use our new function for multi-slice training with mixup
            avg_loss = train_with_multi_slice_mixup(inputs, labels, model, opt)
            batch_loss_values.append(avg_loss)
        else:
            # Single slice processing (unchanged)
            labels = labels[:,np.int32((opt.nslices-1)/2),:,:]
            labels = torch.clamp(labels, 0.001, 0.999).float()
            inputs, labels = mixup(inputs, labels, np.random.beta(mixup_ab, mixup_ab))
            model.set_input_sep(inputs, labels)
            model.optimize_parameters(clip_norm=10.0)
            batch_loss_values.append(model.get_current_errors()['Seg_loss'])
    
    batch_loss_values_cpu = [x.detach().cpu() for x in batch_loss_values]

        
    # if (epoch%opt.display_freq)==0:      
    #                     # save_result = total_steps % opt.update_html_freq == 0
    #                     # visualizer.display_current_results(model.get_current_visuals(), epoch, save_result)
                        # errors = model.get_current_errors()['Seg_loss']
                        
                        # message = '(epoch: %d) ' %epoch
                        # for k, v in errors.items():
                        #     message += '%s: %.3f ' % (k, v)
                        #     print(message)

                        #t = (time.time() - iter_start_time) / opt.batchSize
                        # print (errors)
                        #visualizer.print_current_errors(epoch, epoch_iter, errors, t)    
        
        
    if (epoch%opt.display_freq)==0:
        print("-" * 10)
        print(f"epoch {epoch}/{num_epochs}")  
              
        with torch.no_grad(): # no grade calculation 
            dice_2D=[]
            dice_strings = []
            smooth=1
            step=0
            for val_data in val_loader:
                step += 1
                adc, label_val = val_data["img"].to(device), val_data["seg"].to(device)
        
                label_val_vol=label_val
                #if torch.sum(label_val)>0:
                # adc=scale_ADC(adc)
                
                val_inputs=adc
               
                
                with autocast(enabled=True):
                    #pass model segmentor and region info
                    #input, roi size, sw batch size, model, overlap (0.5)
                    val_data["pred"] = sliding_window_inference(val_inputs,
                                                                (128, 128, opt.nslices),
                                                                1,
                                                                model.netSeg_A,
                                                                overlap=0.66,
                                                                mode="gaussian",
                                                                sigma_scale=[0.128, 0.128,0.01])
            
                seg = from_engine(["pred"])(val_data)
                #print("seg length: ", len(seg))
                seg = seg[0]
                #print("seg shape: ", np.shape(seg))
                seg = np.array(seg)
                seg=np.squeeze(seg)
                seg[seg >= 0.5]=1.0
                seg[seg < 0.5]=0.0
                
                
                
                
               
                # print(np.ndim(seg))
                # print(seg)
                # print(np.shape(seg))
                
                # t2w=t2w[0,:,:,:,2]
                # adc=adc[0,:,:,:,2]
                # label_val=label_val[0,:,:,:,2]
                
                
                # transform = transforms.ToPILImage()
                # t2w = transform(t2w)
                # adc = transform(adc+1)
                # label_png = transform(label_val)
                # t2w.save('t2w.png')
                # adc.save('adc.png')
                # label_png.save('label.png')
                
    
                
                
                seg_temp=np.array(seg)
                gt_temp=np.array(label_val_vol.cpu())
    
                seg_flt=seg_temp.flatten()
                gt_flt=gt_temp.flatten()
                
                
                intersection = np.sum(seg_flt * (gt_flt > 0))
                dice_2D_temp=(2. * intersection + smooth) / (np.sum(seg_flt) + np.sum(gt_flt > 0) + smooth)
                dice_2D.append(dice_2D_temp)
                dice_strings.append(f"Case {step}: {dice_2D_temp:.4f}")
                
                
                
                
            
            avg_dice_2D=np.average(dice_2D)
            print(dice_strings)
            print('epoch %i' % epoch, 'DSC  %.3f' % avg_dice_2D, ' (best: %.3f)'  % best_dice, ' best last epoch: %i' % best_epoch, 'current training epoch loss: %.4f' % np.average(batch_loss_values_cpu))
            
            fd_results.write(str(dice_2D) + '\n')
            fd_results.flush()  
            
                    
            if avg_dice_2D>best_dice:
                    print ('saving for Dice, %.3f' % avg_dice_2D, ' > %.3f' % best_dice) 
                    model.save('AVG_best_finetuned')
                    best_dice = avg_dice_2D
                    best_epoch = epoch
                    im_iter=0
                    for vol_data in val_loader:
                        im_iter += 1
                        adc, label_val = vol_data["img"].to(device), vol_data["seg"].to(device)
                        
                        img_name=adc.meta['filename_or_obj'][0].split('/')[-1]
                        print(img_name)
                        
                        label_val_vol=label_val
                        #if torch.sum(label_val)>0:
                        # adc=scale_ADC(adc)
                        
                        val_inputs=adc
                       
                        
                        with autocast(enabled=True):
                            #pass model segmentor and region info
                            #input, roi size, sw batch size, model, overlap (0.5)
                            vol_data["pred"] = sliding_window_inference(val_inputs,
                                                                        (128, 128, opt.nslices),
                                                                        1,
                                                                        model.netSeg_A,
                                                                        overlap=0.66,
                                                                        mode="gaussian",
                                                                        sigma_scale=[0.128, 0.128,0.01])
                        
                        
                        
                        seg = from_engine(["pred"])(vol_data)
                        #print("seg length: ", len(seg))
                        seg = seg[0]
                        #print("seg shape: ", np.shape(seg))
                        seg = np.array(seg)
                        seg=np.squeeze(seg)
                        seg[seg >= 0.5]=1.0
                        seg[seg < 0.5]=0.0
                    
                        #sitk.WriteImage(sitk.GetImageFromArray(t2w.cpu()), os.path.join(results_path,'val%i_t2w.nii.gz' % im_iter))
                        #sitk.WriteImage(sitk.GetImageFromArray(adc.cpu()),  os.path.join(results_path,'val%i_adc.nii.gz' % im_iter))
                        #sitk.WriteImage(sitk.GetImageFromArray(label_val.cpu()),  os.path.join(results_path,'val%i_gtv.nii.gz' % im_iter))
                        #sitk.WriteImage(sitk.GetImageFromArray(seg),  os.path.join(results_path,'val%i_seg.nii.gz' % im_iter))
                        
                        vol_data = [post_transforms(i) for i in decollate_batch(vol_data)]
                        seg_out= from_engine(["pred"])(vol_data)[0]
                        seg_out = np.array(seg_out)
                        seg_out=np.squeeze(seg_out)
                        seg_out[seg_out >= 0.5]=1.0
                        seg_out[seg_out < 0.5]=0.0
                        seg_out = np.transpose(seg_out, (2, 1, 0))
                        
                        
                        cur_rd_path=os.path.join(valpath,img_name)
                        #im_obj = sitk.ReadImage(cur_rd_path)
                        #seg_out = sitk.GetImageFromArray(seg_out)
                        #seg_out = copy_info(im_obj, seg_out)
                        #sitk.WriteImage(seg_out,  os.path.join(seg_path,'seg_%s' % img_name))



    if lr>0:
            model.update_learning_rate()
